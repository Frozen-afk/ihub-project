{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89d06ee1-1575-425b-b853-c7d3b96be456",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "# import torch\n",
    "\n",
    "model_name = \"openai/clip-vit-base-patch32\"  \n",
    "model = CLIPModel.from_pretrained(model_name)\n",
    "processor = CLIPProcessor.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c84b966-fd13-43f9-be8a-cd94ade6a377",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "gallery_path='gallery/'\n",
    "image_extensions=['.jpg','.jpeg','.png']\n",
    "image_paths=[]\n",
    "for filename in os.listdir(gallery_path):\n",
    "    if any(filename.lower().endswith(ext) for ext in image_extensions):\n",
    "        image_paths.append(os.path.join(gallery_path,filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8929f2ff-203c-43dd-97ac-33b492edf5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "gallery_path='gallery/'\n",
    "image_extensions=['.jpg','.jpeg','.png']\n",
    "image_paths=[]\n",
    "for filename in os.listdir(gallery_path):\n",
    "    if any(filename.lower().endswith(ext) for ext in image_extensions):\n",
    "        image_paths.append(os.path.join(gallery_path,filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b89d7200-a50f-4019-9eea-3d4c88d71dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# from tensorflow import device\n",
    "all_embeddings=[]\n",
    "all_filenames=[]\n",
    "with torch.no_grad():\n",
    "    for img_path in image_paths:\n",
    "        try:\n",
    "            image=Image.open(img_path)\n",
    "            inputs=processor(images=image,return_tensors='pt')\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            image_embedding=model.get_image_features(**inputs)\n",
    "            all_filenames.append(img_path)\n",
    "            \n",
    "            \n",
    "           \n",
    "            \n",
    "            \n",
    "           \n",
    "            all_embeddings.append(image_embedding)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f'Error processing {img_path}:{e}')\n",
    "if all_embeddings:\n",
    "    all_embeddings_tensor=torch.stack(all_embeddings)\n",
    "    all_embeddings_tensor=all_embeddings_tensor.cpu()\n",
    "else:\n",
    "    all_embeddings_tensor=torch.empty(0)\n",
    "index_data={\n",
    "    'embeddings':all_embeddings_tensor,\n",
    "    'filenames':all_filenames\n",
    "}\n",
    "\n",
    "with open('index.pkl','wb') as f:\n",
    "    pickle.dump(index_data,f)           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1100f7f8-47cb-4beb-97b0-99145fb2173c",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open('index.pkl','rb') as f:\n",
    "        index_data=pickle.load(f)\n",
    "except FileNotFoundError:\n",
    "    print(f'index file not found')\n",
    "gallery_embeddings=index_data['embeddings'].to(device)\n",
    "gallery_filenames=index_data['filenames']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3782c1f1-8046-4171-8086-77979472437d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a search query (or type 'q' to quit): dog\n",
      "Enter a search query (or type 'q' to quit): food\n",
      "Enter a search query (or type 'q' to quit): q\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exiting...\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "while True:\n",
    "    search_query=input('Enter a search query (or type \\'q\\' to quit):')\n",
    "    if search_query.lower()=='q':\n",
    "        print('Exiting...')\n",
    "        break\n",
    "    if not search_query:\n",
    "        continue\n",
    "    with torch.no_grad():\n",
    "        inputs=processor(text=search_query,return_tensors='pt').to(device)\n",
    "        text_embedding=model.get_text_features(**inputs)\n",
    "        similarity_scores=F.cosine_similarity(text_embedding,gallery_embeddings)\n",
    "        k=3\n",
    "        top_scores,top_indices=torch.topk(similarity_scores,k=k)\n",
    "        for i in range(k):\n",
    "            score=top_scores[i][0].item()\n",
    "            index=top_indices[i][0].item()\n",
    "            filename=gallery_filenames[index]\n",
    "            try:\n",
    "                img=Image.open(filename)\n",
    "                img.show()\n",
    "            except Exception as e:\n",
    "                print(f'error')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813abc0d-0287-4c4b-91cf-15045d566e35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
